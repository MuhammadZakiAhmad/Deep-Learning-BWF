{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Bytewiseship_DeepLearning_Task26 <h1>"
      ],
      "metadata": {
        "id": "ULGahYDq8sFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Submitted By: Muhammad Zaki Ahmad <h3>"
      ],
      "metadata": {
        "id": "O0qrslXC8kwP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsJDwFZP8jfW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding Optimizers:\n",
        "\n",
        "In deep learning, optimizers are algorithms that adjust the weights and biases of a neural network during the training process. They aim to minimize the loss function and find the optimal set of parameters for the model. Different optimizers have their own update rules and strategies for adjusting the weights. Common optimizers include stochastic gradient descent (SGD), Adam, RMSprop, and Adagrad.\n",
        "\n",
        "Last-layer Activations:\n",
        "\n",
        "The activation function applied to the last layer of a neural network depends on the type of problem being solved. For example, in binary classification tasks, a sigmoid activation function is commonly used to produce a probability output between 0 and 1. For multi-class classification, a softmax activation function is often employed to produce a probability distribution over multiple classes.\n",
        "\n",
        "Loss Functions:\n",
        "\n",
        "Loss functions quantify the dissimilarity between the predicted output and the actual target values in a neural network. They are used to measure how well the model is performing during training. The choice of loss function depends on the nature of the problem being solved. For example, mean squared error (MSE) is used for regression tasks, binary cross-entropy is suitable for binary classification, and categorical cross-entropy is used for multi-class classification.\n",
        "\n",
        "Evaluation Metrics:\n",
        "\n",
        "Evaluation metrics provide a quantitative measure of the model's performance on a given task. They are used to assess how well the model generalizes to unseen data. Common evaluation metrics in classification tasks include accuracy, precision, recall, and F1 score. Accuracy measures the percentage of correctly classified samples, while precision focuses on the proportion of correctly predicted positive samples. Recall calculates the ratio of correctly predicted positive samples out of all actual positive samples. F1 score is the harmonic mean of precision and recall, combining both metrics into a single value."
      ],
      "metadata": {
        "id": "aEr4YRT380wl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-yPLjDul9TCA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}