{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**RNN, LSTM AND GRU**\n",
        "\n",
        "Inn the book, Fran√ßois Cholet explains that Recurrent Neural Networks (RNNs) are a type of neural network architecture that can process sequential data by maintaining an internal memory. They achieve this by introducing loops in the network, allowing information to persist and be propagated throughout the sequence. This looping behavior enables RNNs to learn from past information and make predictions based on context.\n",
        "\n",
        "Long Short-Term Memory (LSTM) networks, a specific type of RNN, are designed to address the vanishing gradient problem that occurs when training traditional RNNs on long sequences. LSTM networks introduce a memory cell, which is capable of selectively remembering or forgetting information over long sequences. This memory cell helps LSTM networks to capture and retain important information over extended periods, allowing them to learn long-term dependencies in data.\n",
        "\n",
        "Gated Recurrent Units (GRUs) are another type of RNN that also addresses the vanishing gradient problem. GRUs have a similar structure to LSTM networks but with a simplified architecture. They combine the memory update and reset gates into a single \"update gate\" and use a \"reset gate\" to control the flow of information from previous time steps. GRUs have been found to perform similarly to LSTMs in many tasks while requiring fewer parameters.\n",
        "\n",
        "Both LSTM and GRU are variants of RNNs that address the limitations of traditional RNNs by allowing them to effectively capture and process sequential information over long sequences. These architectures have proven to be valuable in various applications, such as natural language processing, speech recognition, and time series analysis, where understanding and modeling sequential patterns is crucial."
      ],
      "metadata": {
        "id": "E-KSZutbIi-J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q0g982mzFcVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5GiPN__hFlPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tymmivrHFqwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R91yrQTiFs3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VhALPk6wFv-n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-z2GQ4LFFx_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "YIG4qScyF0sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vBa6HVbEF2fS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}