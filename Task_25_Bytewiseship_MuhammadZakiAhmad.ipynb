{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Underfitting, Overfitting, and Regularization**\n",
        "\n",
        "\n",
        "Underfitting, overfitting, and regularization are important concepts in machine learning and deep learning that help to avoid or mitigate the problems of model performance.\n",
        "\n",
        "Underfitting occurs when a model is too simple to capture the complexity of the underlying data. As a result, the model performs poorly on both the training data and the test data. The solution to underfitting is to increase the complexity of the model, such as adding more layers to a neural network.\n",
        "\n",
        "Overfitting occurs when a model is too complex and starts to fit the training data too closely, to the point that it captures noise in the data rather than the underlying patterns. This results in high training accuracy but poor test accuracy. To avoid overfitting, several techniques can be used such as reducing the complexity of the model, using more training data, and applying regularization techniques.\n",
        "\n",
        "Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function that the model is trying to minimize. There are different types of regularization techniques such as L1 regularization, L2 regularization, and dropout.\n",
        "\n",
        "L1 regularization adds a penalty term proportional to the absolute value of the model weights. This results in sparse models where some weights are set to zero, leading to a simpler model that is less prone to overfitting.\n",
        "\n",
        "L2 regularization adds a penalty term proportional to the square of the model weights. This encourages the model to have smaller weights overall, leading to a smoother decision boundary and better generalization.\n",
        "\n",
        "Dropout is a technique where randomly selected neurons are ignored during training, forcing the network to learn more robust features."
      ],
      "metadata": {
        "id": "HfvDKybpgqRq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9wDLnXHjgffX"
      },
      "outputs": [],
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, a neural network with two hidden layers is created using the Sequential class from Keras. The kernel_regularizer parameter is set to regularizers.l2(0.001) for both hidden layers, which adds an L2 regularization penalty term to the loss function during training."
      ],
      "metadata": {
        "id": "N4CtUX8qg4bk"
      }
    }
  ]
}